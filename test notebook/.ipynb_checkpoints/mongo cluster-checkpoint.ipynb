{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import json\n",
    "from google.cloud import storage\n",
    "from news_data_call import *\n",
    "from user_definition import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from user_definition import *\n",
    "\n",
    "from news_data_call import *\n",
    "from user_definition import *\n",
    "\n",
    "\n",
    "\n",
    "def read_json_from_gcs(bucket_name, blob_name, service_account_key_file):\n",
    "    storage_client = storage.Client.from_service_account_json(service_account_key_file)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    data_string = blob.download_as_text()\n",
    "    data = json.loads(data_string)\n",
    "    return data\n",
    "\n",
    "def gcs_to_mongob(uri,data,sources_name):\n",
    "\n",
    "\n",
    "    client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "    client.admin.command('ping')\n",
    "    # Retrieve news data from Google Cloud Storage\n",
    "    db = client['news']\n",
    "    collection = db[sources_name]\n",
    "    collection.insert_many(data)\n",
    "    return collection\n",
    "\n",
    "\n",
    "uri = \"mongodb+srv://waffleiron:zNkFSHTfTGroBXRq@waffleironcluster.7uzj7t2.mongodb.net/?retryWrites=true&w=majority&appName=WaffleIronCluster\"\n",
    "# Example usage: service_account_key_file, bucket_name, \"waffle.json\"\n",
    "\n",
    "news_data = read_json_from_gcs(bucket_name, \"2024-03-01_cnn.json\", service_account_key_file)\n",
    "data = gcs_to_mongob(uri, news_data, \"2024-03-01_cnn\")\n",
    "\n",
    "def mongod_to_spark(data, sources_name):\n",
    "    data = list(data.find({}))\n",
    "\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MongoDB to DataFrame\") \\\n",
    "        .getOrCreate()\n",
    "    csv_file = f\"{sources_name}.csv\"\n",
    "    headers = data[0].keys()\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "    \n",
    "    # Write column headers\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # Write data rows\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "    dfs = pd.read_csv(f\"{sources_name}.csv\")\n",
    "    dfs = dfs.drop(columns=[\"videos\", \"images\"])\n",
    "    dfs[\"publisher\"] = dfs[\"publisher\"].apply(lambda x: eval(x)['title'])\n",
    "    \n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"top_image\", StringType(), True),\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"short_description\", StringType(), True),\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"publisher\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "    df = spark.createDataFrame(dfs, schema=schema)\n",
    "    df.show()\n",
    "    return df\n",
    "    \n",
    "dfs = mongod_to_spark(data, \"2024-03-01_cnn\")\n",
    "dfs.show()\n",
    "\n",
    "\n",
    "    # Create a DataFrame from the MongoDB data\n",
    "#df = spark.createDataFrame(data)\n",
    "\n",
    "    # Show DataFrame schema and some sample data\n",
    "#df.printSchema()\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fa418",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "dateCounts = dfs.groupBy(\"date\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b06cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pub = dfs.groupBy(\"publisher\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "pub.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c3f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dfs.select([\"text\"])\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc856f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6158a0",
   "metadata": {},
   "source": [
    "### SparkSQL queries in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e298d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view to execute SQL queries\n",
    "data.createOrReplaceTempView(\"articles\")\n",
    "\n",
    "# Perform SQL query\n",
    "query_result = spark.sql(\"SELECT title, text, date FROM articles\")\n",
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f6159",
   "metadata": {},
   "source": [
    "#  Agregations done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4763fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data (Assuming `dfs` is your DataFrame)\n",
    "data = dfs.select([\"text\", \"title\", \"date\", \"short_description\"])\n",
    "data.show()\n",
    "\n",
    "# Define a function to perform sentiment analysis using VADER\n",
    "def analyze_sentiment(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sid.polarity_scores(text)\n",
    "    if sentiment_score['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_score['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Define a UDF to apply sentiment analysis function to DataFrame\n",
    "sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "# Apply sentiment analysis to DataFrame\n",
    "x = sentiment_udf(data[\"text\"])\n",
    "x\n",
    "\n",
    "y = sentiment_udf(data[\"title\"])\n",
    "y\n",
    "data_with_sentiment = data.withColumn(\"sentiment text\", x)\n",
    "\n",
    "data_with_sentiment = data_with_sentiment.withColumn(\"sentiment title\", y)\n",
    "\n",
    "# Show results\n",
    "data_with_sentiment.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf167bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateCounts = data_with_sentiment.groupBy(\"sentiment\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"))\n",
    "dateCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e23c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateCounts2 = data_with_sentiment.groupBy(\"sentiment title\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"))\n",
    "dateCounts2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add labels based on the title column\n",
    "data_with_labels = data.withColumn(\"label\", monotonically_increasing_id())\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, lr])\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = data_with_labels.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Train model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3403b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98afbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow_call import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parse_from_mongo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14417525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "csv_file = \"data.csv\"\n",
    "headers = data[0].keys()\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=headers)\n",
    "    \n",
    "    # Write column headers\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write data rows\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"CSV file has been created successfully!\")\n",
    "\n",
    "dfs = pd.read_csv(\"data.csv\")\n",
    "dfs = dfs.drop(columns=[\"videos\", \"images\"])\n",
    "dfs[\"publisher\"] = dfs[\"publisher\"].apply(lambda x: eval(x)['title'])\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269db6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"top_image\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"short_description\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(dfs, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType\n",
    "schema = StructType([\n",
    "    StructField(\"_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"top_image\", StringType(), True),\n",
    "    StructField(\"images\", ArrayType(StringType()), True),\n",
    "    StructField(\"videos\", ArrayType(StringType()), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"short_description\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"publisher\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    # Show DataFrame schema and some sample data\n",
    "df.printSchema()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.sql.functions import map_concat, col\n",
    "df = df.withColumn(\"images_str\", concat_ws(\",\", \"images\"))\n",
    "df = df.withColumn(\"videos_str\", concat_ws(\",\", \"videos\"))\n",
    "df = df.withColumn(\"publisher_str\", concat_ws(\",\", \"publisher.href\", \"publisher.title\"))\n",
    "#df = df.withColumn(\"publisher_str\", map_concat(col(\"publisher\")))\n",
    "\n",
    "\n",
    "# Drop the original array column\n",
    "df = df.drop(\"images\")\n",
    "df = df.drop(\"videos\")\n",
    "df = df.drop(\"publisher\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymongo dnspython\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install storage\n",
    "json_df = df.toJSON()\n",
    "\n",
    "# Show the first record as JSON\n",
    "print(json_df.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-storage\n",
    "import pickle\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # Return a dictionary representing the object's state for serialization\n",
    "        return {'data': self.data}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Restore the object's state from the provided dictionary during deserialization\n",
    "        self.data = state['data']\n",
    "\n",
    "# Create an instance of MyClass\n",
    "obj = MyClass(data)\n",
    "\n",
    "# Serialize the object using pickle\n",
    "serialized_data = pickle.dumps(obj)\n",
    "\n",
    "# Deserialize the object using pickle\n",
    "deserialized_obj = pickle.loads(serialized_data)\n",
    "\n",
    "# Print the deserialized object\n",
    "print(deserialized_obj.data)  # Output: example data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64105aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"date\", col(\"date\").cast(\"timestamp\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdaf8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_schema = StructType([\n",
    "    StructField(\"_id\", StringType(), nullable=True),\n",
    "    StructField(\"title\", StringType(), nullable=True),\n",
    "    StructField(\"top_image\", StringType(), nullable=True),\n",
    "    StructField(\"url\", StringType(), nullable=True),\n",
    "    StructField(\"date\", StringType(), nullable=True),\n",
    "    StructField(\"short_description\", StringType(), nullable=True),\n",
    "    StructField(\"text\", StringType(), nullable=True),\n",
    "    StructField(\"images_str\", StringType(), nullable=True),\n",
    "    StructField(\"videos_str\", StringType(), nullable=True),\n",
    "    StructField(\"publisher_str\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(df.rdd, schema=new_schema)\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5cbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pyspark\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d71e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
